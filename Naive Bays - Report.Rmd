---
title: "Assigment - Naive Bayes DIY"
author:
  - Daan Plass - Author
  - Luc Heimans - Reviewer
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

#load libraries
```{r message=FALSE}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```


#Data Understanding
id: unique id for a news article
title: the title of a news article
author: author of the news article
text: the text of the article; could be incomplete
label: a label that marks the article as potentially unreliable
1: unreliable
0: reliable



##read dataset
```{r message=FALSE}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv"
rawFN <- read.csv(url)

```

##view full dataset
```{r}
view(rawFN)
```


##view part of dataset
```{r}
head(rawFN)

```

##Change label into type factor
```{r}
class(rawFN$label)
rawFN$label <- rawFN$label %>% factor %>% relevel("1")
class(rawFN$label)

```




##Wordcloud 
```{r}
index1 <- rawFN %>% filter(label == "0")
index2 <- rawFN %>% filter(label == "1")

wordcloud(index1$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(index2$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))


```


#Data Preparation 2, Using a sample of the data to create a model 

TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * 

Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * 



##Taking a random sample of the rawFN data to make the size smaller
  Because the size is too large for some function to perform. 
```{r}
sample_rawFN<- rawFN[1:4000,1:5]

```

#View sample dataset
```{r}
view(sample_rawFN)
```


##Convert text Corpus 

```{r message=FALSE}
rawCorpus1 <- Corpus(VectorSource(sample_rawFN$text))

```


##Make lowercase, remove numbers, revmove punctuation, remove whitespace 
```{r message=FALSE}

cleanCorpus1 <- rawCorpus1 %>% tm_map(tolower) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords()) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(stripWhitespace)

```

##Transform cleaned up texts into matrix
  each word in each article will get its own column each row will be a article. 

```{r message=FALSE}
cleanDTM1 <- cleanCorpus1 %>% DocumentTermMatrix

```



##Create split indices
```{r}
set.seed(1234)
trainIndex1 <- createDataPartition(sample_rawFN$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex1)
```



##Applying split indices 
```{r}
##Applying split indices to dataframe
trainDF1 <- sample_rawFN[trainIndex1, ]
testDF1 <- sample_rawFN[-trainIndex1, ]

# Apply split indices to Corpus
trainCorpus1 <- cleanCorpus1[trainIndex1]
testCorpus1 <- cleanCorpus1[-trainIndex1]

# Apply split indices to DTM
trainDTM1 <- cleanDTM1[trainIndex1, ]
testDTM1 <- cleanDTM1[-trainIndex1, ]

```



##Eliminating infrequent words 
```{r}
freqWords1 <- trainDTM1 %>% findFreqTerms(5)
trainDTM1 <-  DocumentTermMatrix(trainCorpus1, list(dictionary = freqWords1))
testDTM1 <-  DocumentTermMatrix(testCorpus1, list(dictionary = freqWords1))
```


##Applying categorical factors instead of count of words
  If word appears in document then "yes" if not "no"
```{r}
convert_counts1 <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM1 <- dim(trainDTM1)[2]
trainDTM1 <- apply(trainDTM1, MARGIN = 2, convert_counts1)
testDTM1 <- apply(testDTM1, MARGIN = 2, convert_counts1)

head(trainDTM1[,1:10])
```


##Creating and testing the model 
```{r}

nbayesModel1 <-  naiveBayes(trainDTM1, trainDF1$label, laplace = 1)

predVec1 <- predict(nbayesModel1, testDTM1)
confusionMatrix(predVec1, testDF1$label, positive = "1", dnn = c("Prediction", "True"))


```



#Naive Bayes DIY Reviewed

#Setup

#load libraries

```{r message=FALSE}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```


#Data Understanding
id: unique id for a news article
title: the title of a news article
author: author of the news article
text: the text of the article; could be incomplete
label: a label that marks the article as potentially unreliable
1: unreliable
0: reliable

##read dataset
```{r message=FALSE}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv"
rawFN <- read.csv(url)
```

##view full dataset
```{r}
view(rawFN)
```

## Data understanding

##view part of dataset
```{r}
head(rawFN)
```

### REVIEW ADJUSTMENT 1
I have added the command to change the labels into type factors since otherwise the model cannot recognize the label.

##Change label into type factor
```{r}
class(rawFN$label)
rawFN$label <- rawFN$label %>% factor %>% relevel("1")
class(rawFN$label)

```

## Wordcloud 
```{r}
index1 <- rawFN %>% filter(label == "0")
index2 <- rawFN %>% filter(label == "1")

wordcloud(index1$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(index2$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))


```
### REVIEW NOTE 1
a few things become clear; many message with fakenews classification (0), are related to presidents (in more detail; Trump) and someone who has said something.

#Data Preparation 

##Convert text Corpus 

```{r message=FALSE}
rawCorpus <- Corpus(VectorSource(rawFN$text))
inspect(rawCorpus[1:3])
```


##Make lowercase, remove numbers, remove punctuation, remove whitespace 
```{r message=FALSE}

cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords()) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(stripWhitespace)

```

##View cleandata

```{r}
tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3])

```


##Transform cleaned up texts into matrix
  each word in each article will get its own column each row will be a article. 
  
```{r message=FALSE}
cleanDTM <- cleanCorpus %>% DocumentTermMatrix

```




##Create split indices
```{r}
set.seed(1234)
trainIndex <- createDataPartition(rawFN$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```


##Applying split indices to dataframe
```{r}
##Applying split indices to dataframe
trainDF <- rawFN[trainIndex, ]
testDF <- rawFN[-trainIndex, ]

# Apply split indices to Corpus
trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

# Apply split indices to DTM
trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]

```


##Eliminating infrequent words 
```{r}
freqWords <- trainDTM %>% findFreqTerms(5)
trainDTM <-  DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))
```


##Applying categorical factors instead of count of words
  If word appears in document then "yes" if not "no"
  
```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
```



##Creating and testing the model 
```{r}

nbayesModel <-  naiveBayes(trainDTM, trainDF$type, laplace = 1)

predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "1", dnn = c("Prediction", "True"))

```


#Data Preparation 2, Using a sample of the data to create a model 

TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * TESTING * 

Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * Actual Modeling that works * 


##Taking a random sample of the rawFN data to make the size smaller
Because the size is too large for some function to perform well on my computer.

```{r}
sample_rawFN<- rawFN[1:2000,1:5]

```

#View sample dataset
```{r}
view(sample_rawFN)
```


##Convert text Corpus 

```{r message=FALSE}
rawCorpus1 <- Corpus(VectorSource(sample_rawFN$text))

```


##Make lowercase, remove numbers, revmove punctuation, remove whitespace 
```{r message=FALSE}

cleanCorpus1 <- rawCorpus1 %>% tm_map(tolower) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords()) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(stripWhitespace)

```

##Transform cleaned up texts into matrix
Each word in each article will get its own column each row will be a article. 

```{r message=FALSE}
cleanDTM1 <- cleanCorpus1 %>% DocumentTermMatrix

```

### REVIEW ADJUSTEMENT 2
The set.seed command was set to 4321, I have rearranged it to 1234 to match with the seed set for the full data set.
The times was set to 3 whilst it should be 1 since I do not need more partitions than 1.

##Create split indices
## 
```{r}
set.seed(1234)
trainIndex1 <- createDataPartition(sample_rawFN$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex1)
```

### REVIEW ADJUSTEMENT 3
I have added the split indices to the Corpus since this was not included. 

##Applying split indices 
```{r}
##Applying split indices to dataframe
trainDF1 <- sample_rawFN[trainIndex1, ]
testDF1 <- sample_rawFN[-trainIndex1, ]

# Apply split indices to Corpus
trainCorpus1 <- cleanCorpus1[trainIndex1]
testCorpus1 <- cleanCorpus1[-trainIndex1]

# Apply split indices to DTM
trainDTM1 <- cleanDTM1[trainIndex1, ]
testDTM1 <- cleanDTM1[-trainIndex1, ]

```



##Eliminating infrequent words 
```{r}
freqWords1 <- trainDTM1 %>% findFreqTerms(5)
trainDTM1 <-  DocumentTermMatrix(trainCorpus1, list(dictionary = freqWords1))
testDTM1 <-  DocumentTermMatrix(testCorpus1, list(dictionary = freqWords1))
```


##Applying categorical factors instead of count of words
If word appears in document then "yes" if not "no"
  
```{r}
convert_counts1 <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM1 <- dim(trainDTM1)[2]
trainDTM1 <- apply(trainDTM1, MARGIN = 2, convert_counts1)
testDTM1 <- apply(testDTM1, MARGIN = 2, convert_counts1)

head(trainDTM1[,1:10])
```

### REVIEW ADJUSTEMENT 4
I have changed laplace to 1 since I do want to double positive control smoothing.
I have changed testDTM1 to trainDTM1 since otherwise the vectors will not have the same length and thus the model cannot be made.
I have also adjusted the testDF1type command to testDF1label since I want to test for the label, not type.
I have also changed positive = "ham" to positive = "1" since the correct indicator for positive is 1.

##Creating and testing the model 
```{r}

nbayesModel1 <- naiveBayes(trainDTM1, trainDF1$label, laplace = 1)

predVec1 <- predict(nbayesModel1, testDTM1)
confusionMatrix(predVec1, testDF1$label, positive = "1", dnn = c("Prediction", "True"))


```
















